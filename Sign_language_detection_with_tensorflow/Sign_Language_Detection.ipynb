{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic \n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    image.flags.writeable = False                  \n",
    "    results = model.process(image)                 \n",
    "    image.flags.writeable = True                   \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) \n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        cv2.imshow('camera', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Collecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "actions = np.array(['hello', 'good', 'bad'])\n",
    "\n",
    "no_sequences = 30\n",
    "\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m frame_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sequence_length):\n\u001b[0;32m      7\u001b[0m     ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[1;32m----> 8\u001b[0m     image, results \u001b[39m=\u001b[39m mediapipe_detection(frame, holistic)\n\u001b[0;32m      9\u001b[0m     draw_styled_landmarks(image, results)\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m frame_num \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36mmediapipe_detection\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m      2\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB) \n\u001b[0;32m      3\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m                  \n\u001b[1;32m----> 4\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mprocess(image)                 \n\u001b[0;32m      5\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m                   \n\u001b[0;32m      6\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_RGB2BGR) \n",
      "File \u001b[1;32mc:\\Users\\k8934\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\mediapipe\\python\\solutions\\holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    137\u001b[0m   \u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n\u001b[0;32m    161\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\k8934\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    for action in actions:\n",
    "        for sequence in range(no_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "                ret, frame = cap.read()\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build and Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        if logs.get('categorical_accuracy') > 0.95:\n",
    "            print(\"\\nReached 0.95 accuracy so cancelling training\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4/4 [==============================] - 4s 74ms/step - loss: 1.8858 - categorical_accuracy: 0.2632\n",
      "Epoch 2/500\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 5.5836 - categorical_accuracy: 0.2544\n",
      "Epoch 3/500\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 1.6337 - categorical_accuracy: 0.1930\n",
      "Epoch 4/500\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.9671 - categorical_accuracy: 0.0789\n",
      "Epoch 5/500\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.9877 - categorical_accuracy: 0.2456\n",
      "Epoch 6/500\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 1.4124 - categorical_accuracy: 0.3684\n",
      "Epoch 7/500\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 1.2735 - categorical_accuracy: 0.3860\n",
      "Epoch 8/500\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 1.0183 - categorical_accuracy: 0.4649\n",
      "Epoch 9/500\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 1.3580 - categorical_accuracy: 0.4035\n",
      "Epoch 10/500\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 0.9937 - categorical_accuracy: 0.5000\n",
      "Epoch 11/500\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 0.9764 - categorical_accuracy: 0.4825\n",
      "Epoch 12/500\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.8994 - categorical_accuracy: 0.6228\n",
      "Epoch 13/500\n",
      "4/4 [==============================] - 0s 94ms/step - loss: 0.8397 - categorical_accuracy: 0.5614\n",
      "Epoch 14/500\n",
      "4/4 [==============================] - 0s 97ms/step - loss: 0.7683 - categorical_accuracy: 0.6754\n",
      "Epoch 15/500\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 0.7912 - categorical_accuracy: 0.5877\n",
      "Epoch 16/500\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.7055 - categorical_accuracy: 0.7018\n",
      "Epoch 17/500\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 0.6991 - categorical_accuracy: 0.6667\n",
      "Epoch 18/500\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 1.0407 - categorical_accuracy: 0.5175\n",
      "Epoch 19/500\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.9134 - categorical_accuracy: 0.5263\n",
      "Epoch 20/500\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.8291 - categorical_accuracy: 0.6930\n",
      "Epoch 21/500\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.7796 - categorical_accuracy: 0.4912\n",
      "Epoch 22/500\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.7444 - categorical_accuracy: 0.7193\n",
      "Epoch 23/500\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.6479 - categorical_accuracy: 0.8684\n",
      "Epoch 24/500\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.5141 - categorical_accuracy: 0.7632\n",
      "Epoch 25/500\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.5351 - categorical_accuracy: 0.8070\n",
      "Epoch 26/500\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.4431 - categorical_accuracy: 0.8333\n",
      "Epoch 27/500\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.4279 - categorical_accuracy: 0.7807\n",
      "Epoch 28/500\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.7954 - categorical_accuracy: 0.6491\n",
      "Epoch 29/500\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 0.9890 - categorical_accuracy: 0.5789\n",
      "Epoch 30/500\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.8319 - categorical_accuracy: 0.5088\n",
      "Epoch 31/500\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 0.5169 - categorical_accuracy: 0.7193\n",
      "Epoch 32/500\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.4175 - categorical_accuracy: 0.7544\n",
      "Epoch 33/500\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.3926 - categorical_accuracy: 0.6930\n",
      "Epoch 34/500\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.3667 - categorical_accuracy: 0.7719\n",
      "Epoch 35/500\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 9.2342 - categorical_accuracy: 0.4737\n",
      "Epoch 36/500\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 6.6645 - categorical_accuracy: 0.2018\n",
      "Epoch 37/500\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 3.3718 - categorical_accuracy: 0.2632\n",
      "Epoch 38/500\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 2.6565 - categorical_accuracy: 0.3070\n",
      "Epoch 39/500\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 3.0371 - categorical_accuracy: 0.3158\n",
      "Epoch 40/500\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 1.7307 - categorical_accuracy: 0.3158\n",
      "Epoch 41/500\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.6745 - categorical_accuracy: 0.2632\n",
      "Epoch 42/500\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.4807 - categorical_accuracy: 0.2632\n",
      "Epoch 43/500\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.4324 - categorical_accuracy: 0.2632\n",
      "Epoch 44/500\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 1.3170 - categorical_accuracy: 0.3947\n",
      "Epoch 45/500\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 1.1911 - categorical_accuracy: 0.4825\n",
      "Epoch 46/500\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 1.1785 - categorical_accuracy: 0.4649\n",
      "Epoch 47/500\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.9805 - categorical_accuracy: 0.5526\n",
      "Epoch 48/500\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.9559 - categorical_accuracy: 0.5000\n",
      "Epoch 49/500\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.9285 - categorical_accuracy: 0.5351\n",
      "Epoch 50/500\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.8477 - categorical_accuracy: 0.6404\n",
      "Epoch 51/500\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.8019 - categorical_accuracy: 0.6667\n",
      "Epoch 52/500\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 0.7073 - categorical_accuracy: 0.5965\n",
      "Epoch 53/500\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.5747 - categorical_accuracy: 0.7895\n",
      "Epoch 54/500\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.5216 - categorical_accuracy: 0.7895\n",
      "Epoch 55/500\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.7930 - categorical_accuracy: 0.6228\n",
      "Epoch 56/500\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 0.7175 - categorical_accuracy: 0.6667\n",
      "Epoch 57/500\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.6397 - categorical_accuracy: 0.6579\n",
      "Epoch 58/500\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.4828 - categorical_accuracy: 0.7368\n",
      "Epoch 59/500\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 0.4472 - categorical_accuracy: 0.7895\n",
      "Epoch 60/500\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 0.3435 - categorical_accuracy: 0.9386\n",
      "Epoch 61/500\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 0.3347 - categorical_accuracy: 0.9298\n",
      "Epoch 62/500\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.2552 - categorical_accuracy: 0.9474\n",
      "Epoch 63/500\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2042 - categorical_accuracy: 0.9912\n",
      "Reached 0.95 accuracy so cancelling training\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 0.2042 - categorical_accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a7fcae7fd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=500, callbacks=[callbacks])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Try in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245),(255,255,255)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m         sentence \u001b[39m=\u001b[39m sentence[\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m:]\n\u001b[0;32m     46\u001b[0m \u001b[39m# Viz probabilities\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     image \u001b[39m=\u001b[39m prob_viz(res, actions, image, colors)\n\u001b[0;32m     49\u001b[0m cv2\u001b[39m.\u001b[39mrectangle(image, (\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m), (\u001b[39m640\u001b[39m, \u001b[39m40\u001b[39m), (\u001b[39m245\u001b[39m, \u001b[39m117\u001b[39m, \u001b[39m16\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     50\u001b[0m cv2\u001b[39m.\u001b[39mputText(image, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(sentence), (\u001b[39m3\u001b[39m,\u001b[39m30\u001b[39m), \n\u001b[0;32m     51\u001b[0m                cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[39m1\u001b[39m, (\u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m), \u001b[39m2\u001b[39m, cv2\u001b[39m.\u001b[39mLINE_AA)\n",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m, in \u001b[0;36mprob_viz\u001b[1;34m(res, actions, input_frame, colors)\u001b[0m\n\u001b[0;32m      3\u001b[0m output_frame \u001b[39m=\u001b[39m input_frame\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m num, prob \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(res):\n\u001b[1;32m----> 5\u001b[0m     cv2\u001b[39m.\u001b[39mrectangle(output_frame, (\u001b[39m0\u001b[39m,\u001b[39m60\u001b[39m\u001b[39m+\u001b[39mnum\u001b[39m*\u001b[39m\u001b[39m40\u001b[39m), (\u001b[39mint\u001b[39m(prob\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m), \u001b[39m90\u001b[39m\u001b[39m+\u001b[39mnum\u001b[39m*\u001b[39m\u001b[39m40\u001b[39m), colors[num], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     cv2\u001b[39m.\u001b[39mputText(output_frame, actions[num], (\u001b[39m0\u001b[39m, \u001b[39m85\u001b[39m\u001b[39m+\u001b[39mnum\u001b[39m*\u001b[39m\u001b[39m40\u001b[39m), cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[39m1\u001b[39m, (\u001b[39m255\u001b[39m,\u001b[39m255\u001b[39m,\u001b[39m255\u001b[39m), \u001b[39m2\u001b[39m, cv2\u001b[39m.\u001b[39mLINE_AA)\n\u001b[0;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m output_frame\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('camera', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "089125a572dd8da3a95569df3e2ee7c2fa0efece0938a1bf26782ce065292402"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
